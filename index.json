[{"authors":null,"categories":null,"content":"I\u0026rsquo;m Ryo Yoshida, a master\u0026rsquo;s student at the Department of Language and Information Sciences, Graduate School of Arts and Sciences, University of Tokyo. I\u0026rsquo;m working with Yohei Oseki on cognitive modeling, natural language processing, and psycholinguistics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I\u0026rsquo;m Ryo Yoshida, a master\u0026rsquo;s student at the Department of Language and Information Sciences, Graduate School of Arts and Sciences, University of Tokyo. I\u0026rsquo;m working with Yohei Oseki on cognitive","tags":null,"title":"Ryo Yoshida（吉田 遼）","type":"authors"},{"authors":["Ryo Yoshida","Yohei Oseki"],"categories":["International"],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671422354,"objectID":"270168a950c88d1bf8d12d0e25ba9c97","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-oseki-2022-composition/","publishdate":"2023-02-15T05:03:30.726025Z","relpermalink":"/github-pages/publication/yoshida-oseki-2022-composition/","section":"publication","summary":"In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate whether these components---the composition function and the self-attention mechanism---can both induce human-like syntactic generalization. Specifically, we train language models (LMs) with and without these two components with the model sizes carefully controlled, and evaluate their syntactic generalization performance against six test circuits on the SyntaxGym benchmark. The results demonstrated that the composition function and the self-attention mechanism both play an important role to make LMs more human-like, and closer inspection of linguistic phenomenon implied that the composition function allowed syntactic features, but not semantic features, to percolate into subtree representations.","tags":["First Author","Peer-reviewed"],"title":"Composition, Attention, or Both?","type":"publication"},{"authors":["染谷 大河","吉田 遼","中石 海","濱西 祐之介","大関洋平"],"categories":["Domestic"],"content":"","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671422330,"objectID":"40921f6cf4bc3e8c5c0dc1b3f9eb5b46","permalink":"https://yoshiryo0617.github.io/github-pages/publication/someya-etal-2022-chomsky/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/someya-etal-2022-chomsky/","section":"publication","summary":"","tags":["First Author"],"title":"チョムスキー階層とニューラル言語モデル","type":"publication"},{"authors":["吉田 遼","大関 洋平"],"categories":["Domestic"],"content":"","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"8fdd25509ed7a8ae3d9c3050d36cb786","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-oseki-2022-transformer-grammar/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/yoshida-oseki-2022-transformer-grammar/","section":"publication","summary":"","tags":["First Author"],"title":"トランスフォーマー文法","type":"publication"},{"authors":["Ryo Yoshida","Yohei Oseki"],"categories":[],"content":"","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644139852,"objectID":"169fce2b2f415a625cfb5d722f82cb55","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-learning-argument-structures-2022/","publishdate":"2022-02-06T09:30:52.362627Z","relpermalink":"/github-pages/publication/yoshida-learning-argument-structures-2022/","section":"publication","summary":"","tags":[],"title":"Learning Argument Structures with Recurrent Neural Network Grammars","type":"publication"},{"authors":["吉田 遼"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653269652,"objectID":"e2a781c125ad2b0ed14fc2abcf443beb","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-2022-modeling/","publishdate":"2022-05-23T01:34:12.151964Z","relpermalink":"/github-pages/publication/yoshida-2022-modeling/","section":"publication","summary":"","tags":[],"title":"Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars","type":"publication"},{"authors":["Ryo Yoshida","Hiroshi Noji","Yohei Oseki"],"categories":["International"],"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638072962,"objectID":"813ffad460a52533157f88e8e0991748","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2021-modeling/","publishdate":"2021-11-28T04:18:39.651326Z","relpermalink":"/github-pages/publication/yoshida-etal-2021-modeling/","section":"publication","summary":"In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like, and if so, which parsing strategy is most cognitively plausible. In order to address this question, we evaluated three LMs against human reading times in Japanese with head-final left-branching structures: Long Short-Term Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars (RNNGs) with top-down and left-corner parsing strategies as hierarchical models. Our computational modeling demonstrated that left-corner RNNGs outperformed top-down RNNGs and LSTM, suggesting that hierarchical and left-corner architectures are more cognitively plausible than top-down or sequential architectures. In addition, the relationships between the cognitive plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be discussed.","tags":["First Author","Peer-reviewed"],"title":"Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"5b31f200ddeb8f680a63171900e6607a","permalink":"https://yoshiryo0617.github.io/github-pages/project/sakigake/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/github-pages/project/sakigake/","section":"project","summary":"さきがけ「信頼されるAIの基盤技術」\n研究補助員","tags":null,"title":"研究課題「認知・脳情報処理による人間らしい言語処理モデルの開発」","type":"project"},{"authors":["Tatsuki Kuribayashi","Yohei Oseki","Takumi Ito","Ryo Yoshida","Masayuki Asahara","Kentaro Inui"],"categories":["International"],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638078805,"objectID":"0aa09c03637a0858cf29924353f958c1","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kuribayashi-etal-2021-lower/","publishdate":"2021-11-28T05:54:20.294247Z","relpermalink":"/github-pages/publication/kuribayashi-etal-2021-lower/","section":"publication","summary":"In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization ---textitthe lower perplexity a language model has, the more human-like the language model is--- in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.","tags":["Peer-reviewed"],"title":"Lower Perplexity is Not Always Human-Like","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"5ceb4c92a97d9c01df740c2ed3897d40","permalink":"https://yoshiryo0617.github.io/github-pages/award/nlp2021_2/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/github-pages/award/nlp2021_2/","section":"award","summary":"「予測の正確な言語モデルがヒトらしいとは限らない」","tags":null,"title":"Special Committee Award at NLP2021 (domestic conference)（言語処理学会第27回年次大会, 委員特別賞）(14/361=3.8%)","type":"award"},{"authors":null,"categories":null,"content":" ","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"98fb527ee51e118413a1d29e625752c7","permalink":"https://yoshiryo0617.github.io/github-pages/award/nlp2021/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/github-pages/award/nlp2021/","section":"award","summary":"「再帰的ニューラルネットワーク文法による人間の文処理のモデリング」","tags":null,"title":"Young Researcher Award at NLP2021 (domestic conference)（言語処理学会第27回年次大会, 若手奨励賞）(11/247=4.5%)","type":"award"},{"authors":null,"categories":null,"content":" ","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"c12280c55a21c1ffa3bed2716c64baea","permalink":"https://yoshiryo0617.github.io/github-pages/award/master_thesis/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/github-pages/award/master_thesis/","section":"award","summary":"「Human-Like Language Models: Composition, Attention, or Both?」","tags":null,"title":"一高記念賞","type":"award"},{"authors":["吉田 遼","能地 宏","大関 洋平"],"categories":["Domestic"],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"ef38338f5a3d3952b700cbee04836704","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2021-modeling-rnng/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/yoshida-etal-2021-modeling-rnng/","section":"publication","summary":"","tags":["First Author"],"title":"再帰的ニューラルネットワーク文法による人間の文処理のモデリング","type":"publication"},{"authors":["栗林 樹生","大関 洋平","伊藤 拓海","吉田 遼","浅原 正幸","乾 健太郎"],"categories":["Domestic"],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"94f6fdfe6ce1240a45ea8d48906ea200","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kuribayashi-etal-2021-lower-perp/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/kuribayashi-etal-2021-lower-perp/","section":"publication","summary":"","tags":[""],"title":"予測の正確な言語モデルがヒトらしいとは限らない","type":"publication"},{"authors":["栗林 樹生","大関 洋平","伊藤 拓海","吉田 遼","浅原 正幸","乾 健太郎"],"categories":["Domestic"],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"39c4131a633ea8ecf0692a60507feb6c","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kuribayashi-etal-2021-readability/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/kuribayashi-etal-2021-readability/","section":"publication","summary":"","tags":[""],"title":"日本語の読みやすさに対する情報量に基づいた統一的な解釈","type":"publication"},{"authors":["吉田 遼","能地 宏","大関 洋平"],"categories":["Domestic"],"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"f909657a5f33e8786ec538d3e0e81379","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2021-modeling-rnng-ling/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/yoshida-etal-2021-modeling-rnng-ling/","section":"publication","summary":"","tags":["First Author"],"title":"再帰的ニューラルネットワーク文法によるヒト文処理のモデリング","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://yoshiryo0617.github.io/github-pages/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/github-pages/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]