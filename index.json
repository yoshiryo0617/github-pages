[{"authors":null,"categories":null,"content":"I\u0026rsquo;m Ryo Yoshida, a Ph.D student at the Department of Language and Information Sciences, Graduate School of Arts and Sciences, University of Tokyo. I\u0026rsquo;m working with Yohei Oseki on natural language processing, cognitive modeling, and syntactic supervision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I\u0026rsquo;m Ryo Yoshida, a Ph.D student at the Department of Language and Information Sciences, Graduate School of Arts and Sciences, University of Tokyo. I\u0026rsquo;m working with Yohei Oseki on natural language processing, cognitive modeling, and syntactic supervision.","tags":null,"title":"Ryo Yoshida","type":"authors"},{"authors":["Taiga Someya","Ryo Yoshida","Hitomi Yanaka","Yohei Oseki"],"categories":[],"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756798175,"objectID":"12dec53a118d36585ccbfeb42d0155a2","permalink":"https://yoshiryo0617.github.io/github-pages/publication/someya-etal-2025-derivational/","publishdate":"2025-09-02T07:29:35.344022Z","relpermalink":"/github-pages/publication/someya-etal-2025-derivational/","section":"publication","summary":"Recent work has demonstrated that neural language models encode syntactic structures in their internal *representations*, yet the *derivations* by which these structures are constructed across layers remain poorly understood. In this paper, we propose *Derivational Probing* to investigate how micro-syntactic structures (e.g., subject noun phrases) and macro-syntactic structures (e.g., the relationship between the root verbs and their direct dependents) are constructed as word embeddings propagate upward across layers.Our experiments on BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge in lower layers and are gradually integrated into a coherent macro-syntactic structure in higher layers.Furthermore, a targeted evaluation on subject-verb number agreement shows that the timing of constructing macro-syntactic structures is critical for downstream performance, suggesting an optimal timing for integrating global syntactic information.","tags":[],"title":"Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models","type":"publication"},{"authors":["Masato Mita","Ryo Yoshida","Yohei Oseki"],"categories":[],"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756798065,"objectID":"05caecc1c8617c9b3a338b8399fd2e8f","permalink":"https://yoshiryo0617.github.io/github-pages/publication/mita-etal-2025-developmentally/","publishdate":"2025-09-02T07:27:45.481924Z","relpermalink":"/github-pages/publication/mita-etal-2025-developmentally/","section":"publication","summary":"Large language models possess general linguistic abilities but acquire language less efficiently than humans. This study proposes a method for integrating the developmental characteristics of working memory during the critical period, a stage when human language acquisition is particularly efficient, into the training process of language models. The proposed method introduces a mechanism that initially constrains working memory during the early stages of training and gradually relaxes this constraint in an exponential manner as learning progresses. Targeted syntactic evaluation shows that the proposed method outperforms conventional methods without memory constraints or with static memory constraints. These findings not only provide new directions for designing data-efficient language models but also offer indirect evidence supporting the role of the developmental characteristics of working memory as the underlying mechanism of the critical period in language acquisition.","tags":[],"title":"Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition","type":"publication"},{"authors":["Ryo Yoshida","Shinnosuke Isono","Kohei Kajikawa","Taiga Someya","Yushi Sugimoto","Yohei Oseki"],"categories":[],"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756798116,"objectID":"58f699d42d5d680462469cbfe5b61d71","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2025-attention/","publishdate":"2025-09-02T07:28:36.240756Z","relpermalink":"/github-pages/publication/yoshida-etal-2025-attention/","section":"publication","summary":"Recent work in computational psycholinguistics has revealed intriguing parallels between attention mechanisms and human memory retrieval, focusing primarily on vanilla Transformers that operate on token-level representations. However, computational psycholinguistic research has also established that syntactic structures provide compelling explanations for human sentence processing that token-level factors cannot fully account for. In this paper, we investigate whether the attention mechanism of Transformer Grammar (TG), which uniquely operates on syntactic structures as representational units, can serve as a cognitive model of human memory retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis between models and humans. Our experiments demonstrate that TG′s attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer′s, with further analyses revealing independent contributions from both models. These findings suggest that human sentence processing involves dual memory representations---one based on syntactic structures and another on token sequences---with attention serving as the general memory retrieval algorithm, while highlighting the importance of incorporating syntactic structures as representational units.","tags":[],"title":"If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?","type":"publication"},{"authors":["Ryo Yoshida","Yushi Sugimoto","Yohei Oseki"],"categories":[],"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756798181,"objectID":"15bb252e440813d704a0fb2661e2b3a1","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2025-investigating/","publishdate":"2025-09-02T07:29:40.989963Z","relpermalink":"/github-pages/publication/yoshida-etal-2025-investigating/","section":"publication","summary":"In computational psycholinguistics, Merkx and Frank (2021) demonstrated that surprisal values from Transformers exhibit a closer fit to measures of human reading effort than those from Recurrent Neural Networks (RNNs), suggesting that Transformers' attention mechanisms may capture cue-based retrieval-like operations in human sentence processing. Meanwhile, explicit integration of syntactic structures has been shown to improve language models' ability to model human sentence processing---for example, Hale et al. (2018) demonstrated that Recurrent Neural Network Grammars (RNNGs), which integrate RNNs with explicit syntactic structures, account for human brain activities that vanilla RNNs cannot capture. In this paper, we investigate the psychometric predictive power of Composition Attention Grammars (CAGs), which integrate Transformers with explicit syntactic structures, to test whether they provide a better fit to human reading times than both vanilla Transformers and RNNGs. We hypothesized that CAGs' syntactic attention mechanisms capture cue-based retrieval-like operations over syntactic memory representations---operations that may be involved in human sentence processing. The results of our strictly controlled experiments demonstrate that CAGs outperformed vanilla Transformers and RNNGs, suggesting that the syntactic attention mechanisms of CAGs may serve as a mechanistic implementation of cue-based retrieval from syntactic memory.","tags":[],"title":"Investigating Psychometric Predictive Power of Syntactic Attention","type":"publication"},{"authors":["杉本 侑嗣","吉田 遼","鄭 嫣婷","菅野 彰剛","小泉 政利","大関 洋平"],"categories":["Domestic"],"content":"","date":1741910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"bf00e41c3873af5653e886f1b259d129","permalink":"https://yoshiryo0617.github.io/github-pages/publication/sugimoto-etal-2025-meg/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/sugimoto-etal-2025-meg/","section":"publication","summary":"","tags":[""],"title":"BCCWJ-MEG：日本語脳磁図データの構築","type":"publication"},{"authors":["染谷 大河","吉田 遼","谷中 瞳","大関 洋平"],"categories":["Domestic"],"content":"","date":1741910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"c7d3f189753e47623da4923ca721e697","permalink":"https://yoshiryo0617.github.io/github-pages/publication/someya-etal-2025-derivational-nlp/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/someya-etal-2025-derivational-nlp/","section":"publication","summary":"","tags":[""],"title":"Derivational Probing：言語モデルにおける統語構造構築の解明","type":"publication"},{"authors":["吉田 遼","磯野 真之介","梶川 康平","染谷 大河","杉本 侑嗣","大関 洋平"],"categories":["Domestic"],"content":"","date":1741910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"8582feb8eb41cec30b3b27f485ecc15c","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2025-if-attention-nlp/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/yoshida-etal-2025-if-attention-nlp/","section":"publication","summary":"","tags":[""],"title":"アテンションが記憶想起の認知モデルたりうるならば、記憶の表現としては何が妥当か？","type":"publication"},{"authors":["三田 雅人","吉田 遼","深津 聡世","大関 洋平"],"categories":["Domestic"],"content":"","date":1741910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"10ec84f9d526e530412178d693eaff18","permalink":"https://yoshiryo0617.github.io/github-pages/publication/mita-etal-2025-developmentally-nlp/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/mita-etal-2025-developmentally-nlp/","section":"publication","summary":"","tags":[""],"title":"作業記憶の発達的特性が言語獲得の臨界期を形成する","type":"publication"},{"authors":["中石 海","吉田 遼","梶川 康平","福島 孝治","大関 洋平"],"categories":["Domestic"],"content":"","date":1741910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"f7499d246bdd95b93fde88dff9228f07","permalink":"https://yoshiryo0617.github.io/github-pages/publication/nakaishi-etal-2025-power/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/nakaishi-etal-2025-power/","section":"publication","summary":"","tags":[""],"title":"自然言語における冪則と統語構造の関係の再考","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1741564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741564800,"objectID":"2064be62d39ff9b91fb799793375f252","permalink":"https://yoshiryo0617.github.io/github-pages/award/nlp2025/","publishdate":"2025-03-10T00:00:00Z","relpermalink":"/github-pages/award/nlp2025/","section":"award","summary":"「作業記憶の発達的特性が言語獲得の臨界期を形成する」","tags":null,"title":"Best Paper Award at NLP2024 (domestic conference)（言語処理学会第31回年次大会, 最優秀賞）(1/765=0.1%)","type":"award"},{"authors":["Tatsuki Kuribayashi","Ryo Ueda","Ryo Yoshida","Yohei Oseki","Ted Briscoe","Timothy Baldwin"],"categories":[],"content":"","date":1722470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731026174,"objectID":"cb285ae0d1cf98c563f7efdc5250e59b","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kuribayashi-etal-2024-emergent/","publishdate":"2024-11-08T00:36:14.285487Z","relpermalink":"/github-pages/publication/kuribayashi-etal-2024-emergent/","section":"publication","summary":"The world′s languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics.We study word-order universals through a computational simulation with language models (LMs).Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals.It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals.","tags":[],"title":"Emergent Word Order Universals from Cognitively-Motivated Language Models","type":"publication"},{"authors":["Ryo Yoshida","Taiga Someya","Yohei Oseki"],"categories":[],"content":"","date":1722470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731026026,"objectID":"e74263a220045b1a6fec0edbfc50af24","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2024-tree/","publishdate":"2024-11-08T00:33:45.087459Z","relpermalink":"/github-pages/publication/yoshida-etal-2024-tree/","section":"publication","summary":"Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance; however, they have trouble with inference efficiency due to the explicit generation of syntactic structures. In this paper, we propose a new method dubbed tree-planting: instead of explicitly generating syntactic structures, we ``plant″ trees into attention weights of unidirectional Transformer LMs to implicitly reflect syntactic structures of natural language. Specifically, unidirectional Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which inherit the training efficiency from SLMs without changing the inference efficiency of their underlying Transformer LMs. Targeted syntactic evaluations on the SyntaxGym benchmark demonstrated that TPTs, despite the lack of explicit generation of syntactic structures, significantly outperformed not only vanilla Transformer LMs but also various SLMs that generate hundreds of syntactic structures in parallel. This result suggests that TPTs can learn human-like syntactic knowledge as data-efficiently as SLMs while maintaining the modeling space of Transformer LMs unchanged.","tags":[],"title":"Tree-Planted Transformers: Unidirectional Transformer Language Models with Implicit Syntactic Supervision","type":"publication"},{"authors":["Kohei Kajikawa","Ryo Yoshida","Yohei Oseki"],"categories":["International"],"content":"","date":1721779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"52b4e2ac2bede47f7eab03d0f25b0433","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kajikawa-etal-2024-cc/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/kajikawa-etal-2024-cc/","section":"publication","summary":"","tags":["Peer-reviewed"],"title":"Dissociating Syntactic Operations via Composition Count","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1716508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716508800,"objectID":"d07abd56803b07c3fac652cacac3527f","permalink":"https://yoshiryo0617.github.io/github-pages/award/lrec-coling2024/","publishdate":"2024-05-24T00:00:00Z","relpermalink":"/github-pages/award/lrec-coling2024/","section":"award","summary":"\"Targeted Syntactic Evaluations on the Chomsky Hierarchy\"","tags":null,"title":"Best Paper Award at LREC-COLING2024 (international conference) (1/1554=0.06%)","type":"award"},{"authors":["Taiga Someya","Ryo Yoshida","Yohei Oseki"],"categories":["International"],"content":"","date":1716163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"b73e93859d82da3f6f5211c62b7378d4","permalink":"https://yoshiryo0617.github.io/github-pages/publication/someya-etal-2024-target/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/someya-etal-2024-target/","section":"publication","summary":"","tags":["Peer-reviewed"],"title":"Targeted Syntactic Evaluations on the Chomsky Hierarchy","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"61a922a5b1020f4f424ba9b13b30d3d0","permalink":"https://yoshiryo0617.github.io/github-pages/grant/dc2/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/github-pages/grant/dc2/","section":"grant","summary":"200,000 yen / month + 1,500,000 yen April 2024 - March 2026","tags":null,"title":"Japan Society for Promotion of Science (JSPS) Research Fellowship for Young Scientists (DC2)","type":"grant"},{"authors":null,"categories":null,"content":" ","date":1710115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710115200,"objectID":"57b877b5cd2bfe599a1a8e243dbc4ed2","permalink":"https://yoshiryo0617.github.io/github-pages/award/nlp2024/","publishdate":"2024-03-11T00:00:00Z","relpermalink":"/github-pages/award/nlp2024/","section":"award","summary":"「どのような言語モデルが不可能な言語を学習してしまうのか？---語順普遍を例に---」","tags":null,"title":"Outstanding Paper Award at NLP2024 (domestic conference)（言語処理学会第30回年次大会, 優秀賞）(12/599=2.0%)","type":"award"},{"authors":["吉田 遼","染谷 大河","大関 洋平"],"categories":["Domestic"],"content":"","date":1710115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"95eeb30c32b5c1795dfa1252bf1265e1","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2024-tpt/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/yoshida-etal-2024-tpt/","section":"publication","summary":"","tags":["First Author"],"title":"Tree Planted Transformer: 統語的大規模言語モデルの構築に向けて","type":"publication"},{"authors":["栗林樹生","上田 亮","吉田 遼","大関 洋平","Ted Briscoe (MBZUAI)","Timothy Baldwin"],"categories":["Domestic"],"content":"","date":1710115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"d739b56ba1470d02f29126e9824c666b","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kuribayashi-etal-2024-word-order/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/kuribayashi-etal-2024-word-order/","section":"publication","summary":"","tags":[],"title":"どのような言語モデルが不可能な言語を学習してしまうのか？---語順普遍を例に---","type":"publication"},{"authors":["三輪 敬太","吉田 遼","大関 洋平"],"categories":["Domestic"],"content":"","date":1710115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"62f58ce9c1f3dbc87bf90caced004519","permalink":"https://yoshiryo0617.github.io/github-pages/publication/miwa-etal-2024-tokenization/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/miwa-etal-2024-tokenization/","section":"publication","summary":"","tags":[],"title":"工学的性能と人間らしさの関係はトークン分割に依存する","type":"publication"},{"authors":["Yushi Sugimoto","Ryo Yoshida","Hyeonjeong Jeong","Masatoshi Koizumi","Jonathan R. Brennan","Yohei Oseki"],"categories":["International"],"content":"","date":1690848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692079747,"objectID":"693d526cc1b0f84c444517f78d2dfa05","permalink":"https://yoshiryo0617.github.io/github-pages/publication/sugimoto-2023-localizing/","publishdate":"2023-08-15T06:08:41.433178Z","relpermalink":"/github-pages/publication/sugimoto-2023-localizing/","section":"publication","summary":"","tags":["Peer-reviewed"],"title":"Localizing Syntactic Composition with Left-Corner Recurrent Neural Network Grammars","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1688688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688688000,"objectID":"1d343fd7676bca315f70259990fdc5fb","permalink":"https://yoshiryo0617.github.io/github-pages/grant/jasso/","publishdate":"2023-07-07T00:00:00Z","relpermalink":"/github-pages/grant/jasso/","section":"grant","summary":"1,200,000 yen","tags":null,"title":"日本学生支援機構奨学金 特に優れた業績による返還免除（全額免除）","type":"grant"},{"authors":null,"categories":null,"content":" ","date":1680307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680307200,"objectID":"f2c96f9ce4bf27b9c55aa1a5048b05c7","permalink":"https://yoshiryo0617.github.io/github-pages/grant/spring-gx/","publishdate":"2023-04-01T00:00:00Z","relpermalink":"/github-pages/grant/spring-gx/","section":"grant","summary":"180,000 yen / month + 340,000 yen / year\nApril 2023 - March 2026","tags":null,"title":"グリーントランスフォーメーション (GX) を先導する高度人材育成 (SPRING GX)","type":"grant"},{"authors":["梶川 康平","吉田 遼","大関 洋平"],"categories":["Domestic"],"content":"","date":1678752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"99dc3159715d88652d786f21f839717c","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kajikawa-etal-2023-ccg/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/kajikawa-etal-2023-ccg/","section":"publication","summary":"","tags":[""],"title":"CCGによる日本語文処理のモデリング","type":"publication"},{"authors":["染谷 大河","吉田 遼","中石 海","大関 洋平"],"categories":["Domestic"],"content":"","date":1678752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"abc1474c092889dcbfb7d23b1d08ff37","permalink":"https://yoshiryo0617.github.io/github-pages/publication/someya-etal-2023-chomsky/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/someya-etal-2023-chomsky/","section":"publication","summary":"","tags":[""],"title":"チョムスキー階層とニューラル言語モデル","type":"publication"},{"authors":["杉本 侑嗣","吉田 遼","鄭 嫣婷","小泉 政利","Jonathan R. Brennan","大関 洋平"],"categories":["Domestic"],"content":"","date":1678752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"8ab0db5cc224ba9a68a2b5cd8c61d2eb","permalink":"https://yoshiryo0617.github.io/github-pages/publication/sugimoto-etal-2023-lrnng-fmri/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/sugimoto-etal-2023-lrnng-fmri/","section":"publication","summary":"","tags":[""],"title":"左隅型再帰的ニューラルネットワーク文法による日本語 fMRI データのモデリング","type":"publication"},{"authors":["磯野 真之介","梶川 康平","吉田 遼","大関 洋平"],"categories":["Domestic"],"content":"","date":1678752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"8097e0173e9e820ef14767f04a1ac5c1","permalink":"https://yoshiryo0617.github.io/github-pages/publication/isono-etal-2023-mrnng/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/isono-etal-2023-mrnng/","section":"publication","summary":"","tags":["First Author"],"title":"極小主義に動機づけられた統語的教示に基づく言語モデル","type":"publication"},{"authors":["吉田 遼","大関 洋平"],"categories":["Domestic"],"content":"","date":1678752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"63ee0b09f6c24d1129f3cb23d3dbbb5f","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-oseki-2023-composition-attention/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/yoshida-oseki-2023-composition-attention/","section":"publication","summary":"","tags":["First Author"],"title":"統語的構成や自己注意を持つ言語モデルは「人間らしい」のか？","type":"publication"},{"authors":["Ryo Yoshida","Yohei Oseki"],"categories":["International"],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671422354,"objectID":"270168a950c88d1bf8d12d0e25ba9c97","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-oseki-2022-composition/","publishdate":"2023-02-15T05:03:30.726025Z","relpermalink":"/github-pages/publication/yoshida-oseki-2022-composition/","section":"publication","summary":"In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate whether these components---the composition function and the self-attention mechanism---can both induce human-like syntactic generalization. Specifically, we train language models (LMs) with and without these two components with the model sizes carefully controlled, and evaluate their syntactic generalization performance against six test circuits on the SyntaxGym benchmark. The results demonstrated that the composition function and the self-attention mechanism both play an important role to make LMs more human-like, and closer inspection of linguistic phenomenon implied that the composition function allowed syntactic features, but not semantic features, to percolate into subtree representations.","tags":["First Author","Peer-reviewed"],"title":"Composition, Attention, or Both?","type":"publication"},{"authors":["染谷 大河","吉田 遼","中石 海","濱西 祐之介","大関洋平"],"categories":["Domestic"],"content":"","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671422330,"objectID":"40921f6cf4bc3e8c5c0dc1b3f9eb5b46","permalink":"https://yoshiryo0617.github.io/github-pages/publication/someya-etal-2022-chomsky/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/someya-etal-2022-chomsky/","section":"publication","summary":"","tags":["First Author"],"title":"チョムスキー階層とニューラル言語モデル","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1647993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647993600,"objectID":"c12280c55a21c1ffa3bed2716c64baea","permalink":"https://yoshiryo0617.github.io/github-pages/award/master_thesis/","publishdate":"2022-03-23T00:00:00Z","relpermalink":"/github-pages/award/master_thesis/","section":"award","summary":"「Human-Like Language Models: Composition, Attention, or Both?」","tags":null,"title":"一高記念賞（東京大学大学院 総合文化研究科）","type":"award"},{"authors":["吉田 遼","大関 洋平"],"categories":["Domestic"],"content":"","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"8fdd25509ed7a8ae3d9c3050d36cb786","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-oseki-2022-transformer-grammar/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/yoshida-oseki-2022-transformer-grammar/","section":"publication","summary":"","tags":["First Author"],"title":"トランスフォーマー文法","type":"publication"},{"authors":["Ryo Yoshida","Yohei Oseki"],"categories":[],"content":"","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644139852,"objectID":"169fce2b2f415a625cfb5d722f82cb55","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-learning-argument-structures-2022/","publishdate":"2022-02-06T09:30:52.362627Z","relpermalink":"/github-pages/publication/yoshida-learning-argument-structures-2022/","section":"publication","summary":"","tags":[],"title":"Learning Argument Structures with Recurrent Neural Network Grammars","type":"publication"},{"authors":["吉田 遼"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653269652,"objectID":"e2a781c125ad2b0ed14fc2abcf443beb","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-2022-modeling/","publishdate":"2022-05-23T01:34:12.151964Z","relpermalink":"/github-pages/publication/yoshida-2022-modeling/","section":"publication","summary":"","tags":[],"title":"Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars","type":"publication"},{"authors":["Ryo Yoshida","Hiroshi Noji","Yohei Oseki"],"categories":["International"],"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638072962,"objectID":"813ffad460a52533157f88e8e0991748","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2021-modeling/","publishdate":"2021-11-28T04:18:39.651326Z","relpermalink":"/github-pages/publication/yoshida-etal-2021-modeling/","section":"publication","summary":"In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like, and if so, which parsing strategy is most cognitively plausible. In order to address this question, we evaluated three LMs against human reading times in Japanese with head-final left-branching structures: Long Short-Term Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars (RNNGs) with top-down and left-corner parsing strategies as hierarchical models. Our computational modeling demonstrated that left-corner RNNGs outperformed top-down RNNGs and LSTM, suggesting that hierarchical and left-corner architectures are more cognitively plausible than top-down or sequential architectures. In addition, the relationships between the cognitive plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be discussed.","tags":["First Author","Peer-reviewed"],"title":"Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars","type":"publication"},{"authors":["Tatsuki Kuribayashi","Yohei Oseki","Takumi Ito","Ryo Yoshida","Masayuki Asahara","Kentaro Inui"],"categories":["International"],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638078805,"objectID":"0aa09c03637a0858cf29924353f958c1","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kuribayashi-etal-2021-lower/","publishdate":"2021-11-28T05:54:20.294247Z","relpermalink":"/github-pages/publication/kuribayashi-etal-2021-lower/","section":"publication","summary":"In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization ---textitthe lower perplexity a language model has, the more human-like the language model is--- in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.","tags":["Peer-reviewed"],"title":"Lower Perplexity is Not Always Human-Like","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"5ceb4c92a97d9c01df740c2ed3897d40","permalink":"https://yoshiryo0617.github.io/github-pages/award/nlp2021_2/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/github-pages/award/nlp2021_2/","section":"award","summary":"「予測の正確な言語モデルがヒトらしいとは限らない」","tags":null,"title":"Special Committee Award at NLP2021 (domestic conference)（言語処理学会第27回年次大会, 委員特別賞）(14/361=3.8%)","type":"award"},{"authors":null,"categories":null,"content":" ","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"98fb527ee51e118413a1d29e625752c7","permalink":"https://yoshiryo0617.github.io/github-pages/award/nlp2021/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/github-pages/award/nlp2021/","section":"award","summary":"「再帰的ニューラルネットワーク文法による人間の文処理のモデリング」","tags":null,"title":"Young Researcher Award at NLP2021 (domestic conference)（言語処理学会第27回年次大会, 若手奨励賞）(11/247=4.5%)","type":"award"},{"authors":["吉田 遼","能地 宏","大関 洋平"],"categories":["Domestic"],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"ef38338f5a3d3952b700cbee04836704","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2021-modeling-rnng/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/yoshida-etal-2021-modeling-rnng/","section":"publication","summary":"","tags":["First Author"],"title":"再帰的ニューラルネットワーク文法による人間の文処理のモデリング","type":"publication"},{"authors":["栗林 樹生","大関 洋平","伊藤 拓海","吉田 遼","浅原 正幸","乾 健太郎"],"categories":["Domestic"],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"94f6fdfe6ce1240a45ea8d48906ea200","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kuribayashi-etal-2021-lower-perp/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/kuribayashi-etal-2021-lower-perp/","section":"publication","summary":"","tags":[""],"title":"予測の正確な言語モデルがヒトらしいとは限らない","type":"publication"},{"authors":["栗林 樹生","大関 洋平","伊藤 拓海","吉田 遼","浅原 正幸","乾 健太郎"],"categories":["Domestic"],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"39c4131a633ea8ecf0692a60507feb6c","permalink":"https://yoshiryo0617.github.io/github-pages/publication/kuribayashi-etal-2021-readability/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/kuribayashi-etal-2021-readability/","section":"publication","summary":"","tags":[""],"title":"日本語の読みやすさに対する情報量に基づいた統一的な解釈","type":"publication"},{"authors":["吉田 遼","能地 宏","大関 洋平"],"categories":["Domestic"],"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638158330,"objectID":"f909657a5f33e8786ec538d3e0e81379","permalink":"https://yoshiryo0617.github.io/github-pages/publication/yoshida-etal-2021-modeling-rnng-ling/","publishdate":"2021-11-29T03:58:50.674293Z","relpermalink":"/github-pages/publication/yoshida-etal-2021-modeling-rnng-ling/","section":"publication","summary":"","tags":["First Author"],"title":"再帰的ニューラルネットワーク文法によるヒト文処理のモデリング","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://yoshiryo0617.github.io/github-pages/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/github-pages/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]