@inproceedings{kuribayashi-etal-2024-emergent,
 abstract = {The worldâ€²s languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics.We study word-order universals through a computational simulation with language models (LMs).Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals.It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals.},
 address = {Bangkok, Thailand},
 author = {Kuribayashi, Tatsuki  and
Ueda, Ryo  and
Yoshida, Ryo  and
Oseki, Yohei  and
Briscoe, Ted  and
Baldwin, Timothy},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2024.acl-long.781},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 pages = {14522--14543},
 publisher = {Association for Computational Linguistics},
 title = {Emergent Word Order Universals from Cognitively-Motivated Language Models},
 url = {https://aclanthology.org/2024.acl-long.781},
 year = {2024}
}

